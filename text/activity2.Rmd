---
title: "activity2"
output: html_fragment
---
A training sample provides a set of inputs and their corresponding expected, or target result. The difference between the target, $t$, and the output of the feed forward process, $O_1$, is an error, $E$. The goal of backpropogation is to use that error to adjust all of the neural network's weights in a way that minimizes the difference between $t$ and $O_1$.

**Exercise 1**. There are several ways to report error after feeding-forward: the error function, $E$, used here is $E = \frac{1}{2}(t - O_1)^2$. Using the first row of the XOR logic gate's truth table---inputs of $I_1 = 1.00$ and $I_2 = 0.00$, and a target of $t = 1.00$---and the results accessible using the after feed-forward radio button, calculate the value of $E$.

**Exercise 2**. In backpropogation, the error at the output layer is spread across the weights between the layers of nodes. Beginning with the output, the algorithm adjusts the weights between the output layer and the hidden layer, and then adjusts the weights between the hidden layer and the input layer. The result is the network accessible using the after backpropagation radio button. In which direction are the weights changing? Are they more negative/less positive, more positive/less negative, or is there no discernible trend? Explain why your results makes sense.

**Exercise 3**. After backpropagation, a new training sample is used to further train the network. Suppose the next training sample has inputs of $I_1 = 0$ and $I_2 = 1$ and a target of $t = 1$, and that the bias values, $B_1$ and $B_2$, remain fixed at $1.00$. Using the weights accessible from the after backpropagation radio button, determine the value for $O_1$ and the resulting error. Comment on your results.
