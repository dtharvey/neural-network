---
title: "activity3"
output: html_fragment
---
A neural network is a tool for making predictions about new samples given information about existing samples. The neural network learns how to make predictions by presenting it with a set of training samples that have known inputs and outputs, feed-forwarding this information through the neural network, and then using backpropogation to adjust the weights between nodes until the neural network successfully learns how to turn a set of known inputs into predicted outputs. This is what it means to train a neural network.

**Exercise 1**. There are two important training variables: the size of weight adjustments (called the learning rate and defined as $\eta$), and the number of times (epochs) each training sample is passed through the neural network. The figure on the upper right allows you to explore how $\eta$ and epochs affects training using the neural network from the last two tabs and using a single training sample with inputs of $I_1 = 0.600$ and $I_2 = 0.400$, and a known output of $t = 0.200$. Defining error, $E$, as $E = \frac{1}{2}(t - O_1)^2$, what is the effect of changing $\eta$ and/or epochs? What are good choices for $\eta$ and epochs if you want to minimize the error in as few epochs as possible? *Note: One would not actually train a neural network using a single training sample; doing so here, however, is a useful way to isolate the effect of $\eta$ and epochs on training*.

**Exercise 2**. Once a network is trained, we can evaluate its predictive ability by presenting it with test samples that have known inputs and outputs and recording the error. The radio buttons on the bottom right provide five test samples in the form of $I_1$/$I_2$/ $t$. Set the training variables $\eta$ and epochs to your optimum values for Exercise 1. What do you notice about the predicted output and the error for each of the five test samples? Why do you think your see this behavior? Sometimes a network is described as overtrained; what do you think this means?





