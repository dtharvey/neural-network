---
title: "activity3"
output: html_fragment
---
#### Training

There are three important training variables: the number of training samples, $n$, the size of weight adjustments (the learning rate, $\eta$), and how many times the set of training samples passes through the neural network (epochs). The controls on the right allow you to explore how each variable affects training: the sliders for $\eta$ and epochs use a log scale; when $n = 1$ the first row of the XOR logic gate's truth table is the training sample, and when $n = 4$ all four rows of the truth table are training samples. The accompanying figure shows how the error changes with each step through the network. When using four training samples you can overlay the average error for each epoch.

**Exercise 1**. Set $\eta$ to $0.001$, epochs to $10$, and one training sample. Step through epochs from $10$ to $10,000$ and describe how the error changes during training.

**Exercise 2**. Set $\eta$ to $0.001$, epochs to $100$, and one training sample. Step through $\eta$ from $0.001$ to $1$ and describe how the error changes during training.

**Exercise 3**. Set $\eta$ to $0.1$ and epochs to $10,000$. Change $n$ from $1$ to $4$ and escribe how the error changes during training.

#### Testing

After training a network, we evaluate its predictive ability by presenting it with test samples that have known inputs and outputs and record the error. The radio button on the bottom right use the rows in the XOR logic gate's truth table as test samples in the form $I_1 / I_2 / t$.

**Exercise 4**. Return to Exercises 1--3 and examine the predictions and errors for each training sample. What do you notice about your results?





