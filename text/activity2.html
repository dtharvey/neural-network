


<p>A training sample provides a set of inputs and their corresponding
expected, or target result. The difference between the target, <span class="math inline">\(t\)</span>, and the output of the feed forward
process, <span class="math inline">\(O_1\)</span>, is an error, <span class="math inline">\(E\)</span>. The goal of backpropogation is to use
that error to adjust all of the neural network’s weights in a way that
minimizes the difference between <span class="math inline">\(t\)</span>
and <span class="math inline">\(O_1\)</span>.</p>
<p><strong>Exercise 1</strong>. There are several ways to report error
after feeding-forward: the error function, <span class="math inline">\(E\)</span>, used here is <span class="math inline">\(E = \frac{1}{2}(t - O_1)^2\)</span>. Using the
first row of the XOR logic gate’s truth table—inputs of <span class="math inline">\(I_1 = 1.00\)</span> and <span class="math inline">\(I_2 = 0.00\)</span>, and a target of <span class="math inline">\(t = 1.00\)</span>—and the results accessible using
the after feed-forward radio button, calculate the value of <span class="math inline">\(E\)</span>.</p>
<p><strong>Exercise 2</strong>. In backpropogation, the error at the
output layer is spread across the weights between the layers of nodes.
Beginning with the output, the algorithm adjusts the weights between the
output layer and the hidden layer, and then adjusts the weights between
the hidden layer and the input layer. The result is the network
accessible using the after backpropagation radio button. In which
direction are the weights changing? Are they more negative/less
positive, more positive/less negative, or is there no discernible trend?
Explain why your results makes sense.</p>
<p><strong>Exercise 3</strong>. After backpropagation, a new training
sample is used to further train the network. Suppose the next training
sample has inputs of <span class="math inline">\(I_1 = 0\)</span> and
<span class="math inline">\(I_2 = 1\)</span> and a target of <span class="math inline">\(t = 1\)</span>, and that the bias values, <span class="math inline">\(B_1\)</span> and <span class="math inline">\(B_2\)</span>, remain fixed at <span class="math inline">\(1.00\)</span>. Using the weights accessible from
the after backpropagation radio button, determine the value for <span class="math inline">\(O_1\)</span> and the resulting error. Comment on
your results.</p>
