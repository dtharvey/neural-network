---
title: "activity1"
output: html_fragment
---
The table on the upper right is the truth table for the XOR (exclusive or) logic gate, which takes two inputs and produces a single output. Values of 1 are read as true and values of $0$ are read as false. The template below the table is a neural network used to model the behavior of the XOR logic gate; it has two inputs, two nodes in a hidden layer, two bias terms, and a single output. Weights connect nodes and bias terms to each other; thus, $W_{2,1}$ is the weight between $I_2$ and $H_1$, $V_{2,1}$ is the weight between $H_2$ and $O_1$, and $U_{1,2}$ is the weight between $B_1$ and $H_2$. 

**Exercise 1**. The XOR logic gate seems simple but is difficult to model. Plot the four pairs of inputs placing Input 1 on the x-axis and Input 2 on the y-axis, and use a circle for an output of 1 and a square for an output of 0. Using your plot, show that you cannot separate the two outputs from each other using a single straight line.  

**Exercise 2**. Use the radio button to display one possible initial state for the network. Weights are drawn from a random uniform distribution with limits of $\frac{\pm 1}{\sqrt{n}}$, where $n$ is the number of weights that contribute to a node's value. Each node's initial value is a weighted sum of the inputs and weights that feed into it; thus $H_1 = I_1 \times W_{1,1} + I_2 \times W_{21} + B_1 \times U_{1,1} = 0.183$. Why is a weighted sum a reasonable estimate for a node's initial value? Calculate the initial value for $H_2$.

**Exercise 3**. An activation function limits nodes to a narrow range of possible values. A sigmoidal function, $f(x) = 1 / (1 + e^{-x})$, is a common choice where $x$ is a node's weighted sum. Explain why this function limits nodes to values between $0$ and $1$. Applying a sigmoidal activation function to the weighted sum for $H_1$ changes its value from $0.183$ to $0.546$. Calculate the effect of a sigmoidal activation function on $H_2$. 

**Exercise 5**. The feed-forward process for the output layer follows the same rules: first calculate the weighted sum for the output node and then apply a sigmoidal activation function. Calculate the output node's value and check your work by selecting the radio button labeled after feed-forward. 
